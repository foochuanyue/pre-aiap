{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AIAP Banner](../images/AIAP-Banner.png \"AIAP Banner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Assignment 1 - Part 1: \n",
    "<br>\n",
    "Data Cleaning & Feature Engineering</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Name:</h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before starting...\n",
    "\n",
    "### How to structure your answers\n",
    "\n",
    "There are many different ways to approach a problem in AI/ML. When attempting the problems in the notebooks, keep in mind that there is no definite \"correct way\" to handle any problem. Instead, when choosing which method to use, focus on how current literature approaches the problem, how you intend to evaluate the effectiveness of the solution you are proposing and how to improve the solution if necessary. Similarly, for the \"open-ended\" questions, it is important to substantiate your answers with supporting reasons. Be sure to bounce your ideas off each other to see how different people approach the same problem!\n",
    "\n",
    "### Coding conventions\n",
    "\n",
    "Learning good habits and ensuring your code follows a certain convention is very important in an environment where your code will be shared and read by others. Following the standard conventions highlighted in the [PEP-8 document](https://www.python.org/dev/peps/pep-0008/) is a good start.\n",
    "\n",
    "### Reproducible Data Pipeline\n",
    "\n",
    "In the next few sections, you will be creating a data pipeline in a step by step process. At the end of this notebook, you have to combine all of these steps into a Python module named `datapipeline.py` in the [src folder](./src). The module should contain at least a function with the following signature:\n",
    "\n",
    "```python\n",
    "def transform(data_path):\n",
    "  \"\"\"\n",
    "  :param data_path: ......\n",
    "  :return: ......\n",
    "  \"\"\"\n",
    "  return feature_engineered_dataframe\n",
    "```\n",
    "\n",
    "Once complete, you will be able to use this function freely in this notebook or other notebooks. This will ensure consistency in the data transformation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Most machine learning projects follow a typical flow. \n",
    "\n",
    "    - Defining the problem statement\n",
    "    - Identifying an appropriate dataset\n",
    "    - Extracting the relevant data from data sources\n",
    "    - Defining and labelling the dataset\n",
    "    - Data cleaning\n",
    "    - Exploring the dataset, refered to as Exploratory Data Analysis (EDA)\n",
    "    - Feature engineering\n",
    "    - Selecting and training appropriate models\n",
    "    - Evaluating the model\n",
    "    - Repeating the above steps (data identification to model evaluation) until desired performance is achieved\n",
    "    - Deploying the model\n",
    "    - Model maintenance and retraining (if necessary)\n",
    "\n",
    "\n",
    "For this assignment, we will explore clustering in unsupervised learning. Keep this in mind when performing the data preparation steps (data extraction, data cleaning, EDA and feature engineering) in the next sections of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Topics\n",
    "1. SQL query\n",
    "2. Data cleaning\n",
    "3. Exploratory data analysis\n",
    "4. Feature Engineering\n",
    "5. Data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Deliverables\n",
    "1. Jupyter notebook\n",
    "2. Script: `datapipeline.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Extraction\n",
    "\n",
    "The data for this assignment were modified from the US census-income dataset on the UCI dataset repository. **Please do not download the data directly from the UCI repository. Instead, follow the instructions below to query the dataset from our database.** Additionally, please refer to this [link](https://www2.1010data.com/documentationcenter/beta/Tutorials/MachineLearningExamples/CensusIncomeDataSet.html) for a description of the variables.\n",
    "\n",
    "The data are stored in a database. If you are not familiar with databases, please refer to this [link](https://medium.com/@rwilliams_bv/intro-to-databases-for-people-who-dont-know-a-whole-lot-about-them-a64ae9af712) to obtain a high level overview of databases and their related terms.\n",
    "\n",
    "The type of database used is an Azure SQL Server instance. SQL Server is Microsoft's RDBMS product offering which uses a variant of SQL (Structured Query Language) called T-SQL (Microsoft Transact-SQL).\n",
    "There are many resources available online to learn how to write T-SQL and you should be able to find one that fits to your level of understanding. One such resource is on [TutorialsPoint](https://www.tutorialspoint.com/t_sql/index.htm). In addition, you can use Microsoft's [reference pages](https://docs.microsoft.com/en-us/sql/t-sql/language-reference?view=sql-server-ver15) to quickly look up syntax documentation.\n",
    "At a minimum, you should be able to combine and extract data from multiple tables in an efficient manner so that you can complete your assignments.\n",
    "\n",
    "If you have trouble accessing the database, you may have to download the Microsoft ODBC Driver. Follow the download instructions for [Windows](https://docs.microsoft.com/en-us/sql/connect/odbc/windows/system-requirements-installation-and-driver-files?view=sql-server-ver15) or [Mac/Linux](https://docs.microsoft.com/en-us/sql/connect/odbc/linux-mac/installing-the-microsoft-odbc-driver-for-sql-server?view=sql-server-ver15#microsoft-odbc-driver-131-for-sql-server).\n",
    "\n",
    "The data is hosted on an Azure SQL Server with the following details:\n",
    "\n",
    "    server = 'aiap-training.database.windows.net'\n",
    "    database = 'aiap'\n",
    "    username = 'apprentice'\n",
    "    password = 'Pa55w.rd'\n",
    "    driver= '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "\n",
    "There are 3 separate tables, `basic_bio`, `employment`, `residential_tax`. All of these tables have an `id` column that can be used to merge them.\n",
    "\n",
    "\n",
    "The pandas package has a `read_sql_query` function that can be used to access the data. You may require another python package to use this function.\n",
    "\n",
    "#### 2.1. Query all 3 tables from the SQL server and combine them into a single pandas dataframe. Save this dataframe as a `.csv` file on your local computer with the filepath `assignment1/data/raw/data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning\n",
    "\n",
    "Whenever we have a new dataset, we need to inspect and clean the dataset.\n",
    "\n",
    "Some (non-exhaustive) steps in data cleaning are:\n",
    "    - handling missing values\n",
    "    - checking for irrelevant rows and/or columns\n",
    "    - checking for duplicate rows and deciding whether to drop them\n",
    "    \n",
    "    \n",
    "#### 3.1. List the steps you intend to take to clean the data in the markdown cell below. Give a brief explanation for each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Write each data cleaning step as its own function so that they can be reused individually. Ideally, you should organise the functions so that they can be put into their own library. Remember to follow the [PEP8](https://www.python.org/dev/peps/pep-0008/) convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exploratory Data Analysis\n",
    "\n",
    "After the data has been cleaned, we now need to perform EDA to get a better understanding of the dataset. As a start, you can consider looking at the distributions of each variable as well as the correlation between each pair of variables. Please explore further if you find any meaningful insights. You can refer to this [book chapter](https://www.stat.cmu.edu/~hseltman/309/Book/chapter4.pdf) for detailed information on EDA.\n",
    "\n",
    "\n",
    "#### 4.1. Perform EDA on the dataset. Include all figures / statistics you use. Briefly describe the purpose for each EDA step and the finding(s) from each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Feature Engineering\n",
    "\n",
    "Feature engineering is the process of transforming variables so that they are potentially more useful for the task at hand. Domain expertise is extremely useful for feature engineering. Hence, it is always important to try and understand as much as you can about the problem's domain.\n",
    "\n",
    "Apart from using domain knowledge, we can also perform some simple feature engineering by aggregating different categories within a variable.\n",
    "\n",
    "When conducting feature engineering, it is important to keep in mind the task at hand and take reference from the insights that were derived from the EDA. This will ensure that the features that were created will be useful for the task at hand. The main task that you are feature engineering for is unsupervised learning (clustering).\n",
    "\n",
    "#### 5.1. Engineer 3 new features for this dataset. Briefly explain why you chose these features and how you think they could be useful for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Reproducible Data Pipeline\n",
    "\n",
    "#### 6.1. Finally, take all the code you've written and create a Python module named `datapipeline.py` in the [src folder](./src). The module should at least contain the following function. The transform function should take in the absolute path to the data file as a parameter and return a pandas dataframe.\n",
    "\n",
    "```python\n",
    "def transform(data_path):\n",
    "  \"\"\"\n",
    "  :param data_path: ......\n",
    "  :return: ......\n",
    "  \"\"\"\n",
    "  return feature_engineered_dataframe\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Submission\n",
    "\n",
    "\n",
    "#### 7.1. Create a `conda.yml` file at the base of the assignment folder. Add (manually) your required dependencies to the file named `conda.yml` ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Clustering Algorithms\n",
    "\n",
    "Load the '[`A1P2_clustering.ipynb`](A1P2_clustering.ipynb)' notebook and start working from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>End of Assignment 1 - Part 1: Data Cleaning & Feature Engineering</center></h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

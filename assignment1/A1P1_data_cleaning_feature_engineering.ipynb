{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AIAP Banner](../images/AIAP-Banner.png \"AIAP Banner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Assignment 1 - Part 1: \n",
    "<br>\n",
    "Data Cleaning & Feature Engineering</center></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Name:</h3>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Before starting...\n",
    "\n",
    "### How to structure your answers\n",
    "\n",
    "There are many different ways to approach a problem in AI/ML. When attempting the problems in the notebooks, keep in mind that there is no definite \"correct way\" to handle any problem. Instead, when choosing which method to use, focus on how current literature approaches the problem, how you intend to evaluate the effectiveness of the solution you are proposing and how to improve the solution if necessary. Similarly, for the \"open-ended\" questions, it is important to substantiate your answers with supporting reasons. Be sure to bounce your ideas off each other to see how different people approach the same problem!\n",
    "\n",
    "### Coding conventions\n",
    "\n",
    "Learning good habits and ensuring your code follows a certain convention is very important in an environment where your code will be shared and read by others. Following the standard conventions highlighted in the [PEP-8 document](https://www.python.org/dev/peps/pep-0008/) is a good start.\n",
    "\n",
    "### Reproducible Data Pipeline\n",
    "\n",
    "In the next few sections, you will be creating a data pipeline in a step by step process. At the end of this notebook, you have to combine all of these steps into a Python module named `datapipeline.py` in the [src folder](./src). The module should contain at least a function with the following signature:\n",
    "\n",
    "```python\n",
    "def transform(data_path):\n",
    "  \"\"\"\n",
    "  :param data_path: ......\n",
    "  :return: ......\n",
    "  \"\"\"\n",
    "  return feature_engineered_dataframe\n",
    "```\n",
    "\n",
    "Once complete, you will be able to use this function freely in this notebook or other notebooks. This will ensure consistency in the data transformation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "Most machine learning projects follow a typical flow. \n",
    "\n",
    "    - Defining the problem statement\n",
    "    - Identifying an appropriate dataset\n",
    "    - Extracting the relevant data from data sources\n",
    "    - Defining and labelling the dataset\n",
    "    - Data cleaning\n",
    "    - Exploring the dataset, refered to as Exploratory Data Analysis (EDA)\n",
    "    - Feature engineering\n",
    "    - Selecting and training appropriate models\n",
    "    - Evaluating the model\n",
    "    - Repeating the above steps (data identification to model evaluation) until desired performance is achieved\n",
    "    - Deploying the model\n",
    "    - Model maintenance and retraining (if necessary)\n",
    "\n",
    "\n",
    "For this assignment, we will explore clustering in unsupervised learning. Keep this in mind when performing the data preparation steps (data extraction, data cleaning, EDA and feature engineering) in the next sections of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1. Topics\n",
    "1. SQL query\n",
    "2. Data cleaning\n",
    "3. Exploratory data analysis\n",
    "4. Feature Engineering\n",
    "5. Data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2. Deliverables\n",
    "1. Jupyter notebook\n",
    "2. Script: `datapipeline.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Extraction\n",
    "\n",
    "The data for this assignment were modified from the US census-income dataset on the UCI dataset repository. **Please do not download the data directly from the UCI repository. Instead, follow the instructions below to query the dataset from our database.** Additionally, please refer to this [link](https://www2.1010data.com/documentationcenter/beta/Tutorials/MachineLearningExamples/CensusIncomeDataSet.html) for a description of the variables.\n",
    "\n",
    "The data are stored in a database. If you are not familiar with databases, please refer to this [link](https://medium.com/@rwilliams_bv/intro-to-databases-for-people-who-dont-know-a-whole-lot-about-them-a64ae9af712) to obtain a high level overview of databases and their related terms.\n",
    "\n",
    "The type of database used is an Azure SQL Server instance. SQL Server is Microsoft's RDBMS product offering which uses a variant of SQL (Structured Query Language) called T-SQL (Microsoft Transact-SQL).\n",
    "There are many resources available online to learn how to write T-SQL and you should be able to find one that fits to your level of understanding. One such resource is on [TutorialsPoint](https://www.tutorialspoint.com/t_sql/index.htm). In addition, you can use Microsoft's [reference pages](https://docs.microsoft.com/en-us/sql/t-sql/language-reference?view=sql-server-ver15) to quickly look up syntax documentation.\n",
    "At a minimum, you should be able to combine and extract data from multiple tables in an efficient manner so that you can complete your assignments.\n",
    "\n",
    "If you have trouble accessing the database, you may have to download the Microsoft ODBC Driver. Follow the download instructions for [Windows](https://docs.microsoft.com/en-us/sql/connect/odbc/windows/system-requirements-installation-and-driver-files?view=sql-server-ver15) or [Mac/Linux](https://docs.microsoft.com/en-us/sql/connect/odbc/linux-mac/installing-the-microsoft-odbc-driver-for-sql-server?view=sql-server-ver15#microsoft-odbc-driver-131-for-sql-server).\n",
    "\n",
    "The data is hosted on an Azure SQL Server with the following details:\n",
    "\n",
    "    server = 'aiap-training.database.windows.net'\n",
    "    database = 'aiap'\n",
    "    username = 'apprentice'\n",
    "    password = 'Pa55w.rd'\n",
    "    driver= '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "\n",
    "There are 3 separate tables, `basic_bio`, `employment`, `residential_tax`. All of these tables have an `id` column that can be used to merge them.\n",
    "\n",
    "\n",
    "The pandas package has a `read_sql_query` function that can be used to access the data. You may require another python package to use this function.\n",
    "\n",
    "#### 2.1. Query all 3 tables from the SQL server and combine them into a single pandas dataframe. Save this dataframe as a `.csv` file on your local computer with the filepath `assignment1/data/raw/data.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyodbc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199503, 11)\n",
      "(199503, 24)\n",
      "(199503, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>age</th>\n",
       "      <th>education</th>\n",
       "      <th>enroll_in_edu_inst_last_wk</th>\n",
       "      <th>marital_stat</th>\n",
       "      <th>race</th>\n",
       "      <th>sex</th>\n",
       "      <th>country_of_birth_father</th>\n",
       "      <th>country_of_birth_mother</th>\n",
       "      <th>country_of_birth_self</th>\n",
       "      <th>citizenship</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>Bachelors degree(BA AB BS)</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Married-civilian spouse present</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Native- Born in the United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>49</td>\n",
       "      <td>17</td>\n",
       "      <td>11th grade</td>\n",
       "      <td>High school</td>\n",
       "      <td>Never married</td>\n",
       "      <td>Amer Indian Aleut or Eskimo</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Native- Born in the United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>121</td>\n",
       "      <td>33</td>\n",
       "      <td>High school graduate</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Native- Born in the United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>138</td>\n",
       "      <td>44</td>\n",
       "      <td>Masters degree(MA MS MEng MEd MSW MBA)</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Married-civilian spouse present</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Native- Born in the United States</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>153</td>\n",
       "      <td>62</td>\n",
       "      <td>High school graduate</td>\n",
       "      <td>Not in universe</td>\n",
       "      <td>Married-civilian spouse present</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>United-States</td>\n",
       "      <td>Native- Born in the United States</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  age                               education  \\\n",
       "0    0   38              Bachelors degree(BA AB BS)   \n",
       "1   49   17                              11th grade   \n",
       "2  121   33                    High school graduate   \n",
       "3  138   44  Masters degree(MA MS MEng MEd MSW MBA)   \n",
       "4  153   62                    High school graduate   \n",
       "\n",
       "  enroll_in_edu_inst_last_wk                     marital_stat  \\\n",
       "0            Not in universe  Married-civilian spouse present   \n",
       "1                High school                    Never married   \n",
       "2            Not in universe                         Divorced   \n",
       "3            Not in universe  Married-civilian spouse present   \n",
       "4            Not in universe  Married-civilian spouse present   \n",
       "\n",
       "                          race     sex country_of_birth_father  \\\n",
       "0                        White    Male           United-States   \n",
       "1  Amer Indian Aleut or Eskimo    Male           United-States   \n",
       "2                        Black  Female           United-States   \n",
       "3                        White    Male           United-States   \n",
       "4                        White  Female           United-States   \n",
       "\n",
       "  country_of_birth_mother country_of_birth_self  \\\n",
       "0           United-States         United-States   \n",
       "1           United-States         United-States   \n",
       "2           United-States         United-States   \n",
       "3           United-States         United-States   \n",
       "4           United-States         United-States   \n",
       "\n",
       "                         citizenship  \n",
       "0  Native- Born in the United States  \n",
       "1  Native- Born in the United States  \n",
       "2  Native- Born in the United States  \n",
       "3  Native- Born in the United States  \n",
       "4  Native- Born in the United States  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "server = 'aiap-training.database.windows.net'\n",
    "database = 'aiap'\n",
    "username = 'apprentice'\n",
    "password = 'Pa55w.rd'\n",
    "driver= '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+server+';PORT=1433;DATABASE='+database+';UID='+username+';PWD='+ password) as conn:\n",
    "    df1 = pd.read_sql_query(\"SELECT * FROM basic_bio\", conn)\n",
    "    df2 = pd.read_sql_query(\"SELECT * FROM employment\", conn)\n",
    "    df3 = pd.read_sql_query(\"SELECT * FROM residential_tax\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199463, 48)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df1.merge(df2, left_on='id', right_on='id')\n",
    "df = df.merge(df3, left_on='id', right_on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('data/raw', exist_ok=True)  \n",
    "df.to_csv('data/raw/data.csv')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Cleaning\n",
    "\n",
    "Whenever we have a new dataset, we need to inspect and clean the dataset.\n",
    "\n",
    "Some (non-exhaustive) steps in data cleaning are:\n",
    "    - handling missing values\n",
    "    - checking for irrelevant rows and/or columns\n",
    "    - checking for duplicate rows and deciding whether to drop them\n",
    "    \n",
    "    \n",
    "#### 3.1. List the steps you intend to take to clean the data in the markdown cell below. Give a brief explanation for each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Write each data cleaning step as its own function so that they can be reused individually. Ideally, you should organise the functions so that they can be put into their own library. Remember to follow the [PEP8](https://www.python.org/dev/peps/pep-0008/) convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Exploratory Data Analysis\n",
    "\n",
    "After the data has been cleaned, we now need to perform EDA to get a better understanding of the dataset. As a start, you can consider looking at the distributions of each variable as well as the correlation between each pair of variables. Please explore further if you find any meaningful insights. You can refer to this [book chapter](https://www.stat.cmu.edu/~hseltman/309/Book/chapter4.pdf) for detailed information on EDA.\n",
    "\n",
    "\n",
    "#### 4.1. Perform EDA on the dataset. Include all figures / statistics you use. Briefly describe the purpose for each EDA step and the finding(s) from each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Feature Engineering\n",
    "\n",
    "Feature engineering is the process of transforming variables so that they are potentially more useful for the task at hand. Domain expertise is extremely useful for feature engineering. Hence, it is always important to try and understand as much as you can about the problem's domain.\n",
    "\n",
    "Apart from using domain knowledge, we can also perform some simple feature engineering by aggregating different categories within a variable.\n",
    "\n",
    "When conducting feature engineering, it is important to keep in mind the task at hand and take reference from the insights that were derived from the EDA. This will ensure that the features that were created will be useful for the task at hand. The main task that you are feature engineering for is unsupervised learning (clustering).\n",
    "\n",
    "#### 5.1. Engineer 3 new features for this dataset. Briefly explain why you chose these features and how you think they could be useful for clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Reproducible Data Pipeline\n",
    "\n",
    "#### 6.1. Finally, take all the code you've written and create a Python module named `datapipeline.py` in the [src folder](./src). The module should at least contain the following function. The transform function should take in the absolute path to the data file as a parameter and return a pandas dataframe.\n",
    "\n",
    "```python\n",
    "def transform(data_path):\n",
    "  \"\"\"\n",
    "  :param data_path: ......\n",
    "  :return: ......\n",
    "  \"\"\"\n",
    "  return feature_engineered_dataframe\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Submission\n",
    "\n",
    "\n",
    "#### 7.1. Create a `conda.yml` file at the base of the assignment folder. Add (manually) your required dependencies to the file named `conda.yml` ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore Clustering Algorithms\n",
    "\n",
    "Load the '[`A1P2_clustering.ipynb`](A1P2_clustering.ipynb)' notebook and start working from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>End of Assignment 1 - Part 1: Data Cleaning & Feature Engineering</center></h1>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{"cells":[{"cell_type":"markdown","metadata":{},"source":["![AIAP Banner](../images/AIAP-Banner.png \"AIAP Banner\")"]},{"cell_type":"markdown","metadata":{},"source":["<h1><center>Assignment 1 - Part 2:\n","<br>\n","Clustering</center></h1>"]},{"cell_type":"markdown","metadata":{},"source":["<h3>Name of Apprentice:</h3>\n","\n","---"]},{"cell_type":"markdown","metadata":{},"source":["# 1. Introduction\n","\n","#### 1.1 Unsupervised Learning\n","\n","Unsupervised learning refers to a set of machine learning techniques where no target variables (Y) are given. Only the features (X) are available and our job is to find patterns in X. A useful reference for this assignment is [*Hastie and Tibshirani's Elements of Statistical Learning*](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) (page 485 onwards). Please feel free to suggest and use any other references that you find helpful.\n","\n","For the second part of assignment 1, we will be focusing on different clustering methods for tabular data."]},{"cell_type":"markdown","metadata":{},"source":["#### 1.2. Basics of Clustering \n","Clustering statistically groups datapoints into subsets. Datapoints within the same subset or cluster are more closely related to one another compared to datapoints belonging to another cluster. More information on clustering is available from page 501 of [*Hastie and Tibshirani's Elements of Statistical Learning*](https://web.stanford.edu/~hastie/Papers/ESLII.pdf). \n","\n","Brief overview: \n","- Clustering is extremely useful in numerous domains: \n","    - Customer segmentation for personalised product recommendations\n","    - Topic identification to relieve the need to manually vet documents \n","    - Image or geo-spatial segmentation to optimise supply and demand (Gojek does this) \n","    - Maybe most importantly, utilising clustering to get a sense of a dataset before starting to build models from it\n","  \n","- Some examples of clustering algorithms: \n","    - K-Means\n","    - Gaussian Mixture Models for drawing soft clustering boundaries instead of hard ones \n","    - Hierarchical clustering\n","    - DBSCAN for density-based clustering for anomaly detection \n","    \n","- There are 4 key components to take note of when performing clustering\n","    - distance metric (between points or clusters)\n","    - clustering algorithm\n","    - cluster evaluation metric\n","    - method for determining optimal number of clusters"]},{"cell_type":"markdown","metadata":{},"source":["#### 1.3. Topics\n","1. Hierarchical clustering\n","2. K-Means clustering\n","3. GMM clustering\n","4. Dimensionality reduction using principal components"]},{"cell_type":"markdown","metadata":{},"source":["#### 1.4. Deliverables\n","1. Jupyter notebook"]},{"cell_type":"markdown","metadata":{},"source":["# 1. Opening the Black Box of Clustering \n","\n","#### 1.1. In your own words, describe what a proximity matrix is in the context of clustering. *Sometimes, typing values into MS Excel helps with building intuition.*"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 1.2. In your own words, describe some metrics that can be used as a measure of dissimilarity between objects in a dataset?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# 2. Hierarchical Clustering \n","\n","For a detailed description on hierarchical clustering, please refer to [(ESL reference pg. 520)](https://web.stanford.edu/~hastie/Papers/ESLII.pdf) or [here](https://www.cs.princeton.edu/courses/archive/fall18/cos324/files/hierarchical-clustering.pdf) for an overview or introduction to hierarchical clustering. Alternatively, you can also refer to the various videos on YouTube or blogposts that discuss hierarchical clustering to get an overview of the technique. \n","\n","\n","#### 2.1. Implement additional preprocessing or visualisation steps that you feel are necessary to help build meaningful clusters from the data."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 2.2. Randomly sample 25,000 points from the dataset. We will use this smaller dataset for all hierarchical clustering tasks below. Use single linkage and apply hierarchical clustering to the dataset. Visualise the output from the clustering."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 2.3. Experiment with different linkage algorithms. Visualise the resulting trees for average linkage, complete linkage and single linkage. How do we determine which linkage algorithm to use?"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 2.4. How do you infer the optimal number of clusters from the dendrogram?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 2.5. Using the optimal number of clusters, generate descriptive statistics (for relevant features) for each cluster and evaluate whether they form meaningful segments. It may be useful to consider features that were not used in the clustering as well (like categorical features)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 2.6. List two ways to improve the clustering and implement at least one. Track the results of the first iteration and second iteration."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# 3. K-Means and GMM Clustering \n","\n","Two other techniques for clustering are using K-Means and Gaussian Mixture Models (GMM) [(ESL reference pg. 509)](https://web.stanford.edu/~hastie/Papers/ESLII.pdf). This [resource](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/) provides a good visualisation of how the K-Means algorithm works. Use the **sampled dataset** for K-means and GMM.\n","\n","#### 3.1. Implement K-Means clustering on the data, experimenting with a few different values of 'k'. It may be useful to consider how you intend to choose an optimal value of 'k' at this point."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 3.2. Implement a Gaussian Mixture Model on the data, experimenting with different values for the number of components. Similar to K-Means, consider how to choose an optimal number of components."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 3.3. List two ways to improve the clustering and implement at least one for both K-Means and GMM. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 3.4. Describe at least three techniques to validate whether your chosen clusters correspond to meaningful customer segments. You may consider a combination of different visualisation techniques and/or quantitative metrics (refer to ESL for some examples)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 3.5. Use at least one of the techniques you described above to choose the \"best\" set of clusters. Generate descriptive statistics (for relevant features) for each cluster in this set and evaluate whether they form meaningful segments. It may be useful to consider features that were not used in the clustering as well (like the categorical features)."]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# 4. Outliers \n","\n","#### 4.1. Describe the influence, if any, that outliers have on hierarchical clustering, K-Means and GMM models."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 4.2. Do the outliers themselves form clusters?"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# 5. Selecting an appropriate algorithm\n","\n","#### 5.1. Describe some considerations when choosing between K-Means, GMM or Hierarchical Clustering."]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 5.2. Is there a specific algorithm that you would choose for this dataset?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# 6. Dimensionality Reduction using Principal Component Analysis"]},{"cell_type":"markdown","metadata":{},"source":["#### 6.1. Apply PCA on the full pre-processed dataset. A summary on PCA can be found [here](http://setosa.io/ev/principal-component-analysis/)."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 6.2. Create a plot of cumulative explained variance and number of components. Describe how this plot can be used to determine the number of principal components to choose. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 6.3. Generate a scatterplot of the first principal component, PC0, against the second principal component, PC1, coloured by the GMM's predictions on the normalised dataset with outliers removed for the optimal number of components determined in 2.5."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 6.4. How do the values of principal components relate to the columns in the original dataset? State the equation for the first principal component, PC0."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# 7. Other techniques\n","\n","#### 7.1. Look up other clustering (like DBSCAN) and/or dimension reduction /  visualisation (t-SNE) techniques. Give a brief description of the technique and the common use cases."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 7.2. Repeat the steps above for the technique that you're exploring. Compare its performance with the techniques used above."]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["<h1><center>End of Assignment 1 - Part 2: Clustering</center></h1>"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":2}

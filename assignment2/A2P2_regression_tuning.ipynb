{"cells":[{"cell_type":"markdown","metadata":{},"source":["![AIAP Banner](../images/AIAP-Banner.png \"AIAP Banner\")"]},{"cell_type":"markdown","metadata":{},"source":["<h1><center>Assignment 2 - Part 2:\n","<br>\n","Regression, Validation & Hyperparameter Tuning</center></h1>"]},{"cell_type":"markdown","metadata":{},"source":["<h3>Name of Apprentice:</h3>\n","\n","---"]},{"cell_type":"markdown","metadata":{},"source":["# 1. Introduction\n","In the second part of this assignment, we will explore quite a few regression models, methods for model validation and hyperparameter tuning.\n","\n","#### 1.1. Topics\n","1. EDA & Feature Engineering\n","2. Regression models (linear models, decision trees, k-nearest neighbours, boosting, stacked models) & metrics \n","3. Model training\n","4. Feature selection\n","5. Hyperparameter tuning\n","6. L1 & L2 Regularisation\n","\n","\n","#### 1.2. Deliverables\n","1. Jupyter notebook\n","2. Python scripts (some templates are provided)"]},{"cell_type":"markdown","metadata":{},"source":["# 2. Data preparation\n","\n","For this assignment, we will use regression models to predict the price of resale HDB flats. The data is hosted on the same database instance as the previous 2 assignments.\n","\n","    server = 'aiap-training.database.windows.net'\n","    database = 'aiap'\n","    username = 'apprentice'\n","    password = 'Pa55w.rd'\n","    driver= '{ODBC Driver 17 for SQL Server}'\n","    \n","\n","The data is stored in the tables `transactions`, `towns` and `flat_models`."]},{"cell_type":"markdown","metadata":{},"source":["#### 2.1. Extract the data from the SQL server. Perform the necessary steps needed to combine the tables in an SQL query. Save the final table as a `.csv` file on your laptop."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 2.2. Perform data cleaning and EDA. Drop the `flatm_id` and `town_id` columns. The final dataframe should have 12 columns (including the id column)."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 2.3. Engineer at least 3 features."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# 3. Model Validation\n","\n","Model validation is the process of verifying the model's performance. The first step is to choose a metric for evaluating the model.\n","\n","#### 3.1. Describe the differences between MSE, MAE and RMSE and the considerations when choosing between each metric. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 3.2. Explain the purpose of splitting the full dataset into these 3 subsets: train, development/validation and test. State any assumption about the relationship between the 3 datasets."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 3.3. In general, how large (in terms of proportion of the full dataset) should the train / dev / test set be?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 3.4. Describe some considerations when choosing between using train-dev-test split and k-fold cross validation."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 3.5. Perform a train-test split on your dataset. Save both the train and test sets as separate `csv` files."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 3.6. Consolidate the data preprocessing steps in a `datapipeline.py` file in the [src folder](./src). \n","\n","\n","The module should contain the class `Datapipeline` with at least two functions, `transform_train_data` and `transform_test_data`.\n","\n","`transform_train_data` should take in the path to the training data and return two numpy arrays, `X_train` and `y_train`.\n","\n","`transform_test_data` should take in the path to the test data and return two numpy arrays, `X_test` and `y_test`."]},{"cell_type":"markdown","metadata":{},"source":["# 4. Model Training"]},{"cell_type":"markdown","metadata":{},"source":["You may find sklearn's `pipeline` useful for building pipelines so that the models you build will be easier to consume later during meta-model ensembling.\n","\n","We will begin by building the most basic baseline model - a model that predicts the mean value for all data points regardless of the features.\n","\n","#### 4.1 Calculate the mean of the target variables for the train dataset. Evaluate the \"model\" performance on the test set. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 4.2. Train a linear model and evaluate the model's performance on the test set. Make use"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["**4.3. Using `model.py` file as a template, train a k-nearest neighbours regressor and evaluate the model's performance on the test set.**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["**4.3. Using `model.py` file as a template, train a decision tree regressor and evaluate the model's performance on the test set.**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 4.5. Identify the best model from the linear model, k-nearest neighbour and decision tree. We will use this as our improved baseline."]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["# 5. Feature Selection\n","\n","Feature selection is the process of identifying the features which contribute the most to the model's performance and the dropping of features which contribute less. One common method of evaluating feature importance is the use of Recursive Feature Elimination (RFE). RFE is referred to as backwards stepwise regression in linear models.\n","\n","#### 5.1. Describe the RFE algorithm."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 5.2. Using the RFE algorithm, determine the optimal number of features to keep for linear regression and decision tree regressor models.\n","\n","Additional instructions\n","- Use the train set. \n","- Determine the optimal number of features to keep for each regressor. \n","- Report the MSE (on the test set) for each of the k-nearest neighbours and decision tree models with reduced features."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["# 6. Hyperparameter Tuning\n","\n","Most ML algorithms have various parameters that can be changed or tuned to improve model performance. Hyperparameter tuning is an optimisation problem. One of the most common algorithms for optimisation is grid search.\n","\n","#### 6.1. Describe the grid search algorithm. What are some drawbacks of grid search and what other search algorithms are there?"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 6.2. Explain why gradient descent is not used for hyperparameter tuning."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["We will now start to look at hyperparameters for these models. Between the 3 models, k-nearest neighbours probably has the least hyperparameters to tune. Hence, let's start with k-nearest neighbours. As we have limited time, you might not want to try all of these parameters. This is a good time to do some research with respect to what parameters to optimise.\n","\n","#### 6.3. Perform hyperparameter tuning for k-nearest neighbours regressor and decision tree regressor (with all features). Report the metrics (on the test set) for the best model for each algorithm below."]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"markdown","metadata":{},"source":["The linear model also requires tuning, but the current API we are using `linear_model.LinearRegression` lacks the parameters we are looking for. Instead, we will work with elastic nets to build a better linear regression model. We will look at this again in the regularisation section."]},{"cell_type":"markdown","metadata":{},"source":["# 7. Regularisation\n","\n","Regularisation penalises large weights during the model fitting process and forces the model to trade-off between large weights and model accuracy. This is done by introducing an additional term in the loss function that increases the value of the loss function if the weights are increased. L1 and L2 regularisation are common methods.\n","\n","#### 7.1. Explain how regularization (forcing the model to have smaller weights/parameters/coefficients) can help with feature selection. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 7.2. Determine the optimal value of the regularization parameter for a linear model with L1 regularization (LASSO regression). Report the test MSE for the best model."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 7.3. State the coefficients for the linear model with L1 regularization."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["#### 7.4. Repeat 7.2. and 7.3. for a linear model with L2 regularization (ridge regression). Comment on any differences observed between the coefficients of the two models."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{},"source":["<h1><center>End of Assignment 2 - Part 2:<br>Regression, Validation & Hyperparameter Tuning</center></h1>"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"nbformat":4,"nbformat_minor":2}
